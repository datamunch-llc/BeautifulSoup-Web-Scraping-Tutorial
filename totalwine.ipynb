{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Web Scraping with BeautifulSoup4 and Pandas\n",
    "\n",
    "This Jupyter notebook is a simple demo scraping a web page from Total Wine and showcasing the powerful tools used in web\n",
    "scraping to harvest data from the world wide web. \n",
    "\n",
    "To start, we'll import the packages we'll be using in this demo and go over each one briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup\n",
    "import requests "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Starting with Pandas, which we've abbreviated as 'pd', is an open source data analysis and manipulation library.\n",
    "It's key feature is what is called a <b>DataFrame</b>. A DataFrame is an in-memory data structure with indexing and \n",
    "re-indexing, alignment, manipulation, pivoting, and statistical analysis capabilities (and many more!) \n",
    "\n",
    "BeautifulSoup4 is a simple to use library designed to scrape information from web pages. It has an HTML and \n",
    "XML parser included that allows ease of searching and parsing your desires web page. \n",
    "\n",
    "Requests coins itself as \"an elegant and simple HTTP library for Python, built for human beings.\" And they \n",
    "have every right to make such claim. This library allows us to make HTTP requests using python, and it's how\n",
    "we will be obtaining our information to scrape.\n",
    "\n",
    "Now we'll start with the basics, first we're going to make an HTTP request to our desire website and download a webpage.\n",
    "We'll do this by creating a Session object and use that to make a request to our website!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "destination_url = 'https://www.totalwine.com'\n",
    "session = requests.Session()\n",
    "headers = {\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0\"\n",
    "                         \".3770.142 Safari/537.36\"}\n",
    "response = session.get(destination_url, headers=headers)\n",
    "\n",
    "#side note, you can check the status code of the response from the web server.\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<i>Huzzah!</i> We have successfully made a request. Alright, pack your bags, time to go home. Just kidding, we've only \n",
    "started. So what did we do? We:\n",
    " - Set the destination URL\n",
    " - Created a Session object which will make the request for us\n",
    " - Initialized a dictionary with a single key-value pair, which is the header we'll be using to mimic the user-agent of \n",
    " a regular browser. Sometimes website don't like non-human traffic :( \n",
    " - Called the session object and stored the response in a variable, and finally printed the status code to make sure\n",
    " everything was a-okay. \n",
    "\n",
    "Now that we have our webpage, let's begin scraping! \n",
    "\n",
    "First thing's first, our destination URL is set to the homepage of the website. Since we're interested in getting some \n",
    "details about the products they sell, let's change the destination URL of a page that contains information about a \n",
    "product. I'm a tequila kind of guy, so let's take a look at one of those! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "destination_url = 'https://www.totalwine.com/spirits/tequila/blanco-silver/don-julio-1942-tequila/p/38388750?s=919&igrules=true'\n",
    "response = session.get(destination_url, headers=headers)\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Note: Remember, we want to make sure that we received a valid response from the server. The status code helps with \n",
    "that. If you make a request and you see the status code is anything other than a number between 200, you will have to \n",
    "look at the specific status code and found out why your request didn't succeed with the server.</i>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*If you want to see what the page's source code looks like uncomment the print statement, beware, it's long!*\n",
    "\n",
    "Now we have our page, let's start with scraping! So where do we begin? What kind of information can we get from this \n",
    "web-page? Well, basically just about anything that you can see. For this tutorial, I'm just interested in the name,\n",
    "price, and spirits type on the page.\n",
    "\n",
    "Now that leads us to the next question, how do we extract the information from the page? Let's first do a little digging\n",
    "around and see where our information is being contained. Here, we'll take a look at how the name of the spirit is \n",
    "represented on the web page. \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<h1 class=\"productTitle__3XDd9UVh\">DON JULIO 1942 TEQUILA</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is a fairly simple HTML tag for the name of the tequila name. We can use the BeautifulSoup API to find an element\n",
    "based on its tag.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "name = soup.find(\"h1\").text\n",
    "\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we're going to find the price in this document. Let's take a look at what the code looks like containing the price."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "<div id=\"edlpPrice\" style=\"font-size:36px;color:#811F08;line-height:42px;font-family:'Fjalla One', sans-serif\">$139.99</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This one should be easy as well, notice the **id** attribute in the HTML div tag. BeautifulSoup again makes this quite\n",
    "simple for us.  Using the same *find* method, we can use the **id** parameter to search the document by ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "price = soup.find(id=\"edlpPrice\").text\n",
    "\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "And finally, for the spirits type, this should be quick since we know what we're looking for at this point. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<div class=\"detailsTableText__1SvcRdYn\">Tequila</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The HTML here says that this is a button, with a class of \"detailsTableText__1SvcRdYn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spirit_type = soup.find(\"div\", {\"class\" : \"detailsTableText__1SvcRdYn\"}).text\n",
    "print(spirit_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait a second, that's the brand... Not the spirit type! What gives!? Well, class names aren't unique in HTML, so what \n",
    "happened is with calling BeautifulSoup's *find* method, we were able to find the first instance of a div that had a \n",
    "class with a matching name. So what we need to do is find **all** of the divs that use that class, and find the index\n",
    "of the appropriate element located in the list of returned objects in order to get the spirit type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "divs = soup.find_all(\"div\", {\"class\" : \"detailsTableText__1SvcRdYn\"})\n",
    "for div in divs:\n",
    "    print(div.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It appears that the spirit type is the 3rd object in the array of divs with that class name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spirit_type = divs[2].text\n",
    "print(spirit_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Success! We've found all the information we wanted for this tutorial. So now we're going to show you how to collect this\n",
    "information into a neat table-like structure using a **DataFrame** and exporting our DataFrame to a csv, all using\n",
    "pandas!\n",
    "\n",
    "So, we have all of our information in 3 separate variables: *name*, *price*, and *spirit_type*. So now we're going to \n",
    "merge those three into a collection (*namely a list*) and create a Pandas DataFrame out of that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "details = [name, spirit_type, price]\n",
    "df = pd.DataFrame([details], columns = ['Spirit Name', 'Spirit Type', 'Price'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Pandas makes it incredibly easy to work with and structure your data to make it easy to digest. To conclude this \n",
    "tutorial. We'll use Pandas's neat *to_csv* method to write the results of our scrape to a CSV file! This is handy, \n",
    "because it means we can export our results to a portable file, or even use that same file and import it to a database! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('total_wine_scrape.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "That concludes our tutorial using BeautifulSoup, Python, and Pandas to scrape websites. If you enjoyed this tutorial,\n",
    "or would like to provide some constructive feedback please comment! If you have a request for any other \n",
    "related tutorials you would like to see feel free to comment as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
